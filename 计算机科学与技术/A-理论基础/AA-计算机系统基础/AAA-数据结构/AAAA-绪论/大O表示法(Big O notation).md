大O表示法（Big O notation）是用于描述算法时间或空间复杂度渐近行为的数学符号，广泛应用于计算机科学和数学领域。该方法通过确定函数数量级的渐近上界，表征算法执行时间或资源需求随输入规模n增长的速率，忽略低次项和常数因子，重点考察最高阶项。由德国数论学家保罗·巴赫曼于1892年在《解析数论》中首次提出。

其形式化定义为：若存在正常数c和k，使得对所有$n≥k$满足$0≤f(n)≤cg(n)$，则称$f(n)=O(g(n))$，代表函数$f(n)$的增长速率不超过$g(n)$的某个常数倍。常见时间复杂度等级包括$O(1)$（常数时间）、$O(n)$（线性时间）、$O(n²)$（平方时间）、$O(log n)$（对数时间）及$O(n log n)$（线性对数时间）等，遵循加法规则与乘法规则进行复合复杂度分析 。

该方法采用最坏情况时间复杂度原则度量算法性能，通过识别算法中最频繁执行的核心操作，计算其执行次数的数量级，最终推导出渐近时间复杂度表达式。