# 介绍
“拟合”（fitting）指在观测数据上构建参数化数学模型，使模型能够近似描述数据的生成规律。计算机视觉中常见的拟合问题包括直线/平面拟合、基础矩阵/本质矩阵估计、单应性/透视变换求解、以及三维点云的平面拟合等。

鲁棒估计（robust estimation）关注在含有异常值（outliers）或噪声的观测下，如何得到稳定可靠的模型参数。传统最小二乘法在面对异常值时会被强烈破坏，因而需要发展鲁棒方法（如 M-estimators、RANSAC）来抑制或剔除异常数据的影响。


# 数学基础
## 1. 最小二乘法（Least Squares, LS）

### 1.1 问题形式

给定观测对 (${(x*i,y_i)}*{i=1}^N$) 或更一般的设计矩阵 \(A\) 与观测向量 \(b\)，线性最小二乘问题为：

$$
\min_{\mathbf{p}} \|A\mathbf{p} - b\|_2^2.
$$

正规解（若 \(A^TA\) 可逆）：

$$
\mathbf{p}^*=(A^TA)^{-1}A^Tb.
$$

### 1.2 几何理解

最小二乘求解是把观测向量 \(b\) 投影到由列空间 \(\mathcal{C}(A)\) 张成的子空间上，解对应于使残差向量 \(r=b-A\mathbf{p}\) 与 \(\mathcal{C}(A)\) 正交（正规方程 \(A^T r=0\)）。

### 1.3 局限性

- **对异常值极为敏感**：平方误差会放大离群点的影响。
- **模型假设**：线性最小二乘假定观测噪声为 i.i.d. 高斯，若噪声分布偏离，高斯假设不成立时效果下降。
- **几何/代价不一致**：在某些计算机视觉问题中（例如透视变换的像点对），直接对像素坐标最小二乘并非几何上正确（应采用齐次坐标与代价重参数化，如归一化 DLT）。

---

## 总体最小二乘 / 总体最小二乘法（Total Least Squares, TLS）与几何视角

### 2.1 问题动机

普通最小二乘仅考虑观测变量 \(b\) 含噪，而假定设计矩阵 \(A\) 精确。但在许多视觉问题中，\(A\) 亦由测量得到并含噪（如直接使用像点坐标构建的矩阵）。TLS 允许对 \(A\) 和 \(b\) 同时建模噪声。

### 2.2 TLS 表述

寻找最小扰动 \(\Delta A, \Delta b\) 使得 \((A+\Delta A)\mathbf{p}=b+\Delta b\) 且扰动 Frobenius 范数最小：

$$
\min_{\Delta A,\Delta b,\mathbf{p}} \|[\Delta A\ \Delta b]\|_F \quad\text{s.t. } (A+\Delta A)\mathbf{p}=b+\Delta b.
$$

通过把 \([A\ b]\) 做奇异值分解（SVD），TLS 解可从最小奇异值对应的奇异向量导出。

### 2.3 几何视角

TLS 实际上是在寻找一个低秩近似，使得带参数的超平面能穿过扰动后的观测点集合；几何上等价于对观测点做最小正交距离拟合（而非沿某一坐标轴的投影误差）。

### 2.4 局限性与数值注意

- TLS 对离群点仍不鲁棒（会被异常值影响）。
- SVD 计算开销相对高；对大规模问题需考虑分块或迭代方法。

---

## 3. 鲁棒估计的基本思想

鲁棒估计通过修改损失函数或采样策略来降低异常值的影响。常见思想包括：

- **替换代价函数（M-estimators）**：把平方误差替换为对异常值增长更慢的损失（Huber、Cauchy、Tukey 等），对应加权最小二乘的迭代重加权最小二乘（IRLS）实现。优势是平滑且统计效率高；缺点需选择尺度参数且对大比例外点并非总能收敛到正确解。

- **样本分离策略（随机采样）**：如 RANSAC，通过反复采样小模型并验证内点来剔除外点。

- **剪枝/截断方法**：对残差做阈值截断，仅考虑小于阈值的样本来估计。

# 实现
## RANSAC：随机采样一致性（Random Sample Consensus）

### 核心思想
RANSAC 的基本想法是：在含有大量异常值的样本中，随机选取最小样本集（minimal sample）来拟合候选模型，随后统计该模型的内点数量（在某阈值下的样本），重复若干次后选择内点数最多或代价最低的模型，并以其内点重新估计最终模型。

### 算法流程（伪代码）

1. 给定模型类型（及最小样本数 s）、内点阈值 ($\tau$)、最大迭代次数或置信度要求。
2. 重复直到达到迭代次数或置信度：  
   a. 随机从数据中抽取 s 个样本，估计模型参数。  
   b. 计算所有样本到模型的误差（几何距离或重投影误差），判定哪些为内点（误差 < ($\tau$)）。  
   c. 若当前内点数超过历史最好，则保存该模型并（可选）用内点重估模型参数。
3. 输出包含最多内点的模型并用其内点做最终精修（例如用最小二乘在内点上迭代求解）。

### 关键参数与设定策略

- **最小样本数 s**：由模型自由度决定（例如单应性 4 对点 → s=4；基础矩阵估计 7 或 8 点法 → s=7/8）。
- **内点阈值 \(\tau\)**：与观测噪声、图像尺度、误差度量有关；通常需通过经验或按噪声估计设定。阈值过小会漏掉真实内点，过大则容易把外点包含进内点集合。
- **迭代次数 / 置信度**：给定内点比率估计 \(w\)（如果未知可保守设定），为获得至少一次选到纯内点样本的概率为 \(p\)，需的迭代次数 \(N\) 满足：

  $$
  1 - p = (1 - w^s)^N \quad\Rightarrow\quad N = \frac{\log(1-p)}{\log(1-w^s)}.
  $$

  若 \(w\) 未知，可以保守小值迭代，或在运行中利用当前最大内点比率动态调整 \(N\)。

- **模型得分函数**：除内点计数外，可用加权内点得分（例如以残差反函数加权）以区分内点质量。

### RANSAC 的改进与变体

- **LO-RANSAC / Local Optimization**：在找到潜在好模型后，局部优化（用内点重估、剔除边界点、再次采样）提升精度。
- **PROSAC**：基于样本质量排序逐步偏向更好样本的采样策略，加速收敛。
- **MSAC / MLESAC**：使用基于残差的更细粒度评分函数（代替二值内点计数）。

### 优缺点总结
- 优点：极强的抗外点能力；实现简单且在视觉任务中非常有效。
- 缺点：随机性导致结果不确定；在低内点率时需要大量迭代；阈值与参数调优依赖经验；对模型复杂度敏感（高自由度模型难以在有限样本下抽到纯内点）。

---

## Hough 变换 vs RANSAC（简要对比）

- **方法类型**：Hough 属于参数空间的投票方法（全局累加），RANSAC 属于随机采样-内点验证方法（局部拟合+统计）。
- **复杂度与维度**：霍夫随着参数维度增长（例如圆的 3 维）计算与内存开销迅速上升；RANSAC 随模型自由度增长需要更大最小样本数与迭代数。 
- **对断裂/稀疏支持的鲁棒性**：霍夫因为累积证据能容忍断裂与部分遮挡；RANSAC 在内点率极低时效率下降但在低维模型上效果优良。 
- **实施细节**：霍夫依赖累加器离散化与峰值检测，需要处理量化误差；RANSAC 依赖恰当的内点阈值与置信度设定并常与最小二乘精修结合。

**选用建议**：
- 若要检测图像中的显著参数化几何形状（长直线、圆弧）且能接受累加器开销，优先采用霍夫（或其概率/梯度约束变体）。
- 若要在大量错误匹配或噪声中估计模型参数并需精确拟合，优先采用 RANSAC 并对内点做最小二乘精修。

