循环神经网络（Recurrent Neural Network, RNN）是一种用于处理序列数据的神经网络架构。与传统的前馈神经网络不同，RNN具有循环连接，允许信息在网络的时间步之间传递。这使得RNN特别适合处理时间序列数据、自然语言处理和其他涉及顺序信息的任务。
全连接神经网络模型无法结合上下文去训练模型，因此循环神经网络孕育而生。
RNN对具有**序列特性**的数据非常有效，它能挖掘数据中的时序信息以及语义信息，利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。
循环神经网络的结构并不复杂，从上到下分别为输出层（L）、隐藏层（绿色框和H）、输入层（X）
![[循环神经网络-1.png]]
单看每一列纵向，其实就是全连接神经网络。其X中是一个向量，也就是某个字或词的特征向量，作为输入层，如上图也就是3维向量，W<sub>ax</sub>是输入层到隐藏层的参数矩阵，在上图中其维度就是3X4，绿色框是隐藏层的向量，如上图维度就是4，W<sub>ha</sub>是隐藏层到输出层的参数矩阵，在上图中就是4X2，L是输出层的向量，在上图中维度为2。
而从横向维度看其实就是按照时间线展开，W其实是每个时间点之间的权重矩阵，我们注意到，RNN之所以可以解决序列问题，是因为它可以记住每一时刻的信息，每一时刻的隐藏层不仅由该时刻的输入层决定，还由上一时刻的隐藏层决定，公式如下，其中 Ot 代表t时刻的输出, St 代表t时刻的隐藏层的值：
$$O_t = g(V·S_t)$$
$$S_t = f(U·X_t+W·S_{t-1})$$
**<div style="text-align: center;">S<sub>t</sub>的值不仅仅取决于X<sub>t</sub>，还取决于S<sub>t-1</sub></div>**
值得注意的一点是，在整个训练过程中，每一时刻所用的都是同样的W